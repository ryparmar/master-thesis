{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import random\n",
    "import transformers\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "if '/home/ryparmar/experimental-martin/pretraining/src/' not in sys.path:\n",
    "    sys.path.append('/home/ryparmar/experimental-martin/pretraining/src')\n",
    "\n",
    "import util, io_util, eval\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model import Encoder as Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.mode = 'pretraining'\n",
    "        self.task = 'BFS+ICT'\n",
    "#         self.claims_path = \"/mnt/data/factcheck/CTK/par5/ctk-data\"\n",
    "#         self.articles_path = \"/mnt/data/factcheck/CTK/par5/interim/ctk_filtered.db\"\n",
    "#         self.articles_chunks_path = '/mnt/data/factcheck/ict_chunked_data/ids-chunks-288-pretraining-ctk_filtered.pkl' \n",
    "        self.claims_path = \"/mnt/data/factcheck/fever/data-cs/fever-data\"  #\"/mnt/data/factcheck/fever/data-cs/fever-data\"\n",
    "        self.articles_path = \"/mnt/data/factcheck/fever/data-cs/fever/fever.db\"  #\"/mnt/data/factcheck/CTK/par4/interim/ctk_filtered.db\"\n",
    "        self.articles_chunks_path = '/mnt/data/factcheck/ict_chunked_data/ids-chunks-288-pretraining-wiki_cs.pkl' #\n",
    "        self.model_weight = \"/home/ryparmar/trained_models/debug.w\"\n",
    "        self.bert_model = \"bert-base-multilingual-cased\"\n",
    "        self.learning_rate = 1e-5\n",
    "        self.max_seq = 288\n",
    "        self.epoch = 1\n",
    "        self.bs = 64\n",
    "        self.test_bs = 64\n",
    "        self.remove_prob = 0.9\n",
    "        self.use_cuda = True if torch.cuda.is_available() else False\n",
    "        self.devices = \"0\" if torch.cuda.is_available() else \"\"\n",
    "        self.continue_training = \"/home/ryparmar/trained_models/mbert_wiki_pre_10ep-bfs_10ep-ict_1e-5_288_best\"  #False\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    def add(self, name, val):\n",
    "        if name == 'cls_token_id':\n",
    "            self.cls_token_id = val\n",
    "        if name == 'pad_token_id':\n",
    "            self.pad_token_id = val\n",
    "        if name == 'device':\n",
    "            self.device = val\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "\n",
    "def instantiate_model(config, tokenizer):\n",
    "    configure_devices(config)\n",
    "    model = Model(config)\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0)\n",
    "    metrics = None\n",
    "\n",
    "    if config.continue_training:\n",
    "        state_dict = torch.load(config.continue_training, map_location='cpu')\n",
    "        model.load_state_dict(state_dict['model'])\n",
    "        if 'optimizer_state_dict' in state_dict:\n",
    "            optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = config.learning_rate\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loaded model:\\nEpochs: {state_dict['epoch']}\\nLoss: {state_dict['loss']}\\n\", \n",
    "                  f\"Recall: {state_dict['rec']}\\nMRR: {state_dict['mrr']}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if config.use_cuda:\n",
    "        model = model.cuda()\n",
    "        optimizer_to(optimizer, config.device)\n",
    "        model = torch.nn.DataParallel(model, device_ids=config.devices)\n",
    "    return model, optimizer, metrics\n",
    "\n",
    "\n",
    "def configure_devices(config):\n",
    "    config.devices = [int(device) for device in range(torch.cuda.device_count())]\n",
    "    config.device = config.devices[0] if config.use_cuda else \"cpu\"\n",
    "\n",
    "\n",
    "def get_loader(data, batch_size):\n",
    "    data = TensorDataset(data)\n",
    "    return DataLoader(data,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      sampler=None, drop_last=True)\n",
    "\n",
    "def ids2docs(ids, id2doc: dict):\n",
    "    return [id2doc[int(i)] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "config.add('cls_token_id', tokenizer.encode(tokenizer.cls_token, add_special_tokens=False)[0])\n",
    "config.add('pad_token_id', tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok(x):\n",
    "    print(tokenizer.convert_ids_to_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:\n",
      "Epochs: [0, 1, 2, 3]\n",
      "Loss: [0.16748802861127624, 0.15736810512863084, 0.1498402876436489, 0.14569516037571323]\n",
      " Recall: [0.440744, 0.450195, 0.377738, 0.528953]\n",
      "MRR: [0.259687, 0.266514, 0.216023, 0.319852]\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, metrics = instantiate_model(config, tokenizer)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = eval.Metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = util.make_chunks(config.articles_path, tokenizer, config, save_chunks=True)\n",
    "# doc_chunks = list of documents; document = list of chunks; \n",
    "# chunk = list of sentences; sentence = list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(doc_chunks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309706 3 6\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_chunks), len(doc_chunks[ids[0]]), len(doc_chunks[ids[0]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z', '##lín', '15', '.', 'srpna', '(', 'Č', '##T', '##K', ')', '-', 'Kraj', '##ský', 'sou', '##d', 've', 'Z', '##lín', '##ě', 'dok', '##on', '##čuje', 'dok', '##azo', '##vání', 'v', 'kor', '##up', '##ční', 'ka', '##uze', 'kolem', 'fina', '##nční', '##ho', 'ú', '##řadu', 'v', 'Kromě', '##říž', '##i', '.']\n"
     ]
    }
   ],
   "source": [
    "tok(doc_chunks[ids[0]][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.nested_list_len(doc_chunks[ids[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dict of chunks into list of all chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = [chunk for doc, chunks in doc_chunks.items() for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143110"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kromě', 'prof', '##esi', '##on', '##álních', 'astronom', '##ů', 'se', 'astronomi', '##i', 'v', '##ěn', '##uje', 'i', 'řada', 'astronom', '##ů', 'amat', '##ér', '##ských', '.']\n"
     ]
    }
   ],
   "source": [
    "tok(doc_chunks[6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding chunks...: 100%|██████████| 451629/451629 [00:14<00:00, 30760.37it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_chunks = util.make_chunks(\"/mnt/data/factcheck/fever/data-cs/fever/fever.db\", \n",
    "                                tokenizer, config, as_eval=True, save_chunks=True)\n",
    "dev_articles_ids = list(dev_chunks.keys())\n",
    "dev_chunks, dev_chunks_mask = util.process_chunks(dev_chunks, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9999 claims from dev split.\n"
     ]
    }
   ],
   "source": [
    "claims_dev, evidence_dev, labels_dev = util.load_claims('dev', config,\n",
    "                                             path='/mnt/data/factcheck/fever/data-cs/fever-data/dev.jsonl')\n",
    "claims_dev, claims_dev_mask = util.process_claims(claims_dev, tokenizer, config, _pad_max=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the documents\n",
    "c = 0\n",
    "sdev_ch, sdev_m = {}, {}\n",
    "for k, v in dev_chunks.items():\n",
    "    sdev_ch[k] = v\n",
    "    sdev_m[k] = dev_chunks_mask[k]\n",
    "    c+=1\n",
    "    if c == 1000:\n",
    "        break\n",
    "    \n",
    "# print(type(sdev_ch), type(sdev_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating chunks embeddings...: 100%|██████████| 451629/451629 [00:04<00:00, 91534.08it/s] \n",
      "Embedding given chunks...: 100%|██████████| 7057/7057 [38:54<00:00,  3.02it/s]\n",
      "Embedding given chunks...: 100%|██████████| 157/157 [00:51<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_claim_embeddings, eval_document_embeddings = eval.evaluation_preprocessing(claims_dev, claims_dev_mask, \n",
    "                                                                                dev_chunks, dev_chunks_mask, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 512) (451629, 512)\n"
     ]
    }
   ],
   "source": [
    "print(eval_claim_embeddings.shape, eval_document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sammy Cahn'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_dev[1][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array(['Sammy Cahn', 'Sammy', 'Cahn', 'Sammy Cahn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.where(predicted == evidence_dev[1][0][0][2])[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = [np.where(predicted_evidence == ev)[0][-1] for ev in evidence if ev in predicted_evidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating evaluation metrics: 9999it [57:35,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.055573\tPrecision@20: 0.029298\tRecall@20: 0.538554\tMRR@20: 0.34086650690207226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kk = 20\n",
    "precision, recall, f1, mrr = retriever_score(eval_document_embeddings,dev_articles_ids, eval_claim_embeddings, \n",
    "                                            evidence_dev, labels_dev, config, k=kk)\n",
    "print(f\"F1: {f1}\\tPrecision@{kk}: {precision}\\tRecall@{kk}: {recall}\\tMRR@{kk}: {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ( get_loader(torch.tensor([i for i in range(len(claims_train))]), config.bs) \n",
    "               if config.mode == 'finetuning'\n",
    "               else get_loader(torch.tensor([i for i in range(len(doc_chunks))]), config.bs))\n",
    "\n",
    "id2doc = {i: doc_id for i, (doc_id, _) in enumerate(doc_chunks.items())} if isinstance(doc_chunks, dict) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "batch = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 678107,  893890,  116736,  719527,  500809,  313594,  528943,  928537,\n",
       "        1350184, 1197479,  672918, 2300976, 1733225,  502688,  913880,  184345,\n",
       "        1088734,  450399, 1796903,   79222,  886595, 1352382, 1331826,  370023,\n",
       "          37339, 1208541, 1469085, 1892516, 1020169,  303151,  358553,  423013])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids2docs(batch, id2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 287]) torch.Size([32, 287])\n",
      "tensor([[  101, 64121, 44254,  ...,     0,     0,     0],\n",
      "        [  101, 10685, 24204,  ...,     0,     0,     0],\n",
      "        [  101, 87631,   112,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 23488, 13341,  ...,     0,     0,     0],\n",
      "        [  101, 53068, 10333,  ...,     0,     0,     0],\n",
      "        [  101, 14074, 17513,  ...,     0,     0,     0]]) tensor([[  101,   294, 13188,  ...,     0,     0,     0],\n",
      "        [  101, 28096, 10738,  ...,     0,     0,     0],\n",
      "        [  101, 21416, 10193,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 10469, 11798,  ...,     0,     0,     0],\n",
      "        [  101, 23837, 10738,  ...,     0,     0,     0],\n",
      "        [  101, 42392, 10413,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "batch = batch[0]\n",
    "query, query_mask, context, context_mask = util.get_pretraining_batch(ids2docs(batch, id2doc), doc_chunks, \n",
    "                                                                            tokenizer, config)\n",
    "print(f\"{query.shape} {context.shape}\")\n",
    "print(f\"{query} {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(1):\n",
    "    model.train()\n",
    "    batch_num = len(loader)\n",
    "    num_training_examples, running_loss = 0, 0.0\n",
    "    for batch in tqdm(loader, total=batch_num):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch[0]\n",
    "        num_training_examples += batch.size(0)\n",
    "        if config.mode == 'finetuning':\n",
    "            query, query_mask, \\\n",
    "            context, context_mask = util.get_finetuning_batch(batch, claims_train, claims_train_mask, evidence_train,\n",
    "                                                            doc_chunks, chunks_mask, articles_ids, config)\n",
    "        else:\n",
    "            query, query_mask, \\\n",
    "            context, context_mask = util.get_pretraining_batch(ids2docs(batch, id2doc), doc_chunks, \n",
    "                                                                        tokenizer, config)\n",
    "\n",
    "        query_cls_out = model(x=query, x_mask=query_mask)\n",
    "        context_cls_out = model(x=context, x_mask=context_mask)\n",
    "        logit = torch.matmul(query_cls_out, context_cls_out.transpose(-2, -1))\n",
    "        correct_class = torch.tensor([i for i in range(len(query))]).long().to(config.device)\n",
    "        loss = loss_fn(logit, correct_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch.size(0)\n",
    "        epoch_avg_loss = running_loss / num_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_json = io_util.load_json(wiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs = util.transform_wiki(wiki_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars, par_ids = io_util.load_db(ctk_path, limit=500000) #TODO REMOVE  # returns paragraphs and paragraph ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pars), len(par_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_pars(pars, par_ids):\n",
    "    ret_pars, ret_par_ids = [], []\n",
    "    for pid, p in tqdm(zip(par_ids, pars)):\n",
    "        if not p.strip().endswith('...') and p.strip().endswith('.'):\n",
    "            ret_pars.append(p)\n",
    "            ret_par_ids.append(pid)\n",
    "    return ret_pars, ret_par_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars, par_ids = remove_invalid_pars(pars, par_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pars), len(par_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id, par_id  = par_ids[4].split('_')\n",
    "print(doc_id, par_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = util.transform_ctk(pars, par_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[doc_id][par_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenized = util.tokenize_documents(docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_tokenized[doc_id][par_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in docs_tokenized[doc_id][par_id]:\n",
    "    print(tokenizer.convert_ids_to_tokens(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((f\"#chunks: {len(docs_tokenized[doc_id])}\\n\",\n",
    "       f\"#sentences in paragraph {par_id}: {len(docs_tokenized[doc_id][par_id])}\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = util.create_chunks(docs_tokenized, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(doc_chunks[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ch in enumerate(doc_chunks[doc_id]):\n",
    "    print(f\"chunk: {i}\")\n",
    "    for s in ch:\n",
    "        print(tokenizer.convert_ids_to_tokens(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.task.upper() == 'ICT' and config.mode == 'pretraining': \n",
    "    doc_chunks = util.flatten_chunks(doc_chunks)\n",
    "    print(doc_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ictc, icts = ict_pretraining_targets_and_contexts([0,1,2], flat_chunks, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(data, batch_size):\n",
    "    data = TensorDataset(data)\n",
    "    return DataLoader(data,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      sampler=None, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2doc = {i: doc_id for i, (doc_id, _) in enumerate(doc_chunks.items())} if isinstance(doc_chunks, dict) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, s = bfs_pretraining_targets_and_contexts([0,1,2], [[]] + chunks, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(c[1]), '\\n', tokenizer.convert_ids_to_tokens(s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = util.create_chunks(docs_tokenized, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, titles = util.make_chunks(fever_path, tokenizer, config, save_chunks=False)\n",
    "\n",
    "dev_articles_ids = titles\n",
    "dev_chunks, dev_masks = util.process_chunks(chunks, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dev, evidence_dev, labels_dev = util.load_claims('dev', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dev, claims_dev_mask = util.process_claims(claims_dev, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(claims_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ( get_loader(torch.tensor([i for i in range(len(claims_train))]), config.batch_size) \n",
    "                    if config.mode == 'finetuning' \n",
    "                    else get_loader(torch.tensor([i for i in range(len(chunks))]), config.batch_size) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "batch = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, target_mask, context, context_mask = util.get_pretraining_batch(batch, chunks, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
